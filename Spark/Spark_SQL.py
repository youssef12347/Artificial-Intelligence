from pyspark import SparkContext, SparkConf
from pyspark.sql import SparkSession

sc = SparkContext.getOrCreate(SparkConf().setMaster("local[*]"))

spark = SparkSession \
    .builder \
    .getOrCreate()

from pyspark.sql import Row

df = spark.createDataFrame([Row(id=1, value='value1'),Row(id=2, value='value2')])

# let's have a look what's inside
df.show()

# let's print the schema
df.printSchema()

#Now we register this DataFrame as query table and issue an SQL statement against it. 
#Please note that the result of the SQL execution returns a new DataFrame we can work with.

# register dataframe as query table
df.createOrReplaceTempView('df_view')

# execute SQL query
df_result = spark.sql('select value from df_view where id=2')

#Â examine contents of result
df_result.show()

# get result as string
print(df_result.first().value)

#count nb of rows
print(df.count())